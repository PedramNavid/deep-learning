{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 21102610 length text with vocabulary of: 72\n"
     ]
    }
   ],
   "source": [
    "from model import CharRNN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "\n",
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability\n",
    "epochs = 20\n",
    "print_every_n = 100\n",
    "save_every_n = 200\n",
    "\n",
    "# Read text\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    text = text.lower()\n",
    "    text = \"\".join(filter(lambda x: x in string.printable, text))\n",
    "    \n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "\n",
    "print(\"Read %s length text with vocabulary of: %s\" % (len(text), len(vocab)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Batches\n",
    "def get_batches(arr, batch_size, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x n_steps from arr.\n",
    "\n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    chars_per_batch = batch_size * n_steps\n",
    "    n_batches = len(arr) // chars_per_batch\n",
    "\n",
    "    arr = arr[:n_batches * chars_per_batch]\n",
    "\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n + n_steps]\n",
    "        y_temp = arr[:, n + 1:n + n_steps + 1]\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 100...  Training loss: 2.9903...  0.3047 sec/batch\n",
      "Epoch: 1/20...  Training Step: 200...  Training loss: 2.4421...  0.3056 sec/batch\n",
      "Epoch: 1/20...  Training Step: 300...  Training loss: 2.2373...  0.3058 sec/batch\n",
      "Epoch: 1/20...  Training Step: 400...  Training loss: 2.0642...  0.3056 sec/batch\n",
      "Epoch: 1/20...  Training Step: 500...  Training loss: 1.9435...  0.3058 sec/batch\n",
      "Epoch: 1/20...  Training Step: 600...  Training loss: 1.8561...  0.3057 sec/batch\n",
      "Epoch: 1/20...  Training Step: 700...  Training loss: 1.8078...  0.3059 sec/batch\n",
      "Epoch: 1/20...  Training Step: 800...  Training loss: 1.7450...  0.3064 sec/batch\n",
      "Epoch: 1/20...  Training Step: 900...  Training loss: 1.6692...  0.3053 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1000...  Training loss: 1.6491...  0.3061 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1100...  Training loss: 1.6070...  0.3051 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1200...  Training loss: 1.6719...  0.3058 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1300...  Training loss: 1.6054...  0.3059 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1400...  Training loss: 1.5994...  0.3070 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1500...  Training loss: 1.5751...  0.3062 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1600...  Training loss: 1.5503...  0.3060 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1700...  Training loss: 1.6017...  0.3063 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1800...  Training loss: 1.5750...  0.3053 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1900...  Training loss: 1.4983...  0.3063 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2000...  Training loss: 1.5011...  0.3063 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2100...  Training loss: 1.5129...  0.3054 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2200...  Training loss: 1.4695...  0.3059 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2300...  Training loss: 1.4813...  0.3063 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2400...  Training loss: 1.4671...  0.3066 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2500...  Training loss: 1.4263...  0.3066 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2600...  Training loss: 1.4522...  0.3054 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2700...  Training loss: 1.4534...  0.3065 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2800...  Training loss: 1.4732...  0.3068 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2900...  Training loss: 1.4523...  0.3059 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3000...  Training loss: 1.4265...  0.3056 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3100...  Training loss: 1.4327...  0.3058 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3200...  Training loss: 1.4380...  0.3064 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3300...  Training loss: 1.3803...  0.3054 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3400...  Training loss: 1.3786...  0.3061 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3500...  Training loss: 1.3898...  0.3054 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3600...  Training loss: 1.3721...  0.3048 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3700...  Training loss: 1.4142...  0.3052 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3800...  Training loss: 1.3783...  0.3067 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3900...  Training loss: 1.4145...  0.3059 sec/batch\n",
      "Epoch: 2/20...  Training Step: 4000...  Training loss: 1.3855...  0.3075 sec/batch\n",
      "Epoch: 2/20...  Training Step: 4100...  Training loss: 1.3658...  0.3063 sec/batch\n",
      "Epoch: 2/20...  Training Step: 4200...  Training loss: 1.3097...  0.3064 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4300...  Training loss: 1.3606...  0.3056 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4400...  Training loss: 1.3497...  0.3065 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4500...  Training loss: 1.3416...  0.3074 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4600...  Training loss: 1.3568...  0.3069 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4700...  Training loss: 1.3341...  0.3061 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4800...  Training loss: 1.3460...  0.3067 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4900...  Training loss: 1.3592...  0.3060 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5000...  Training loss: 1.3303...  0.3060 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5100...  Training loss: 1.3346...  0.3062 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5200...  Training loss: 1.3459...  0.3052 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5300...  Training loss: 1.3405...  0.3060 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5400...  Training loss: 1.2999...  0.3061 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5500...  Training loss: 1.3385...  0.3063 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5600...  Training loss: 1.3418...  0.3059 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5700...  Training loss: 1.3497...  0.3052 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5800...  Training loss: 1.3347...  0.3056 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5900...  Training loss: 1.3737...  0.3056 sec/batch\n",
      "Epoch: 3/20...  Training Step: 6000...  Training loss: 1.3211...  0.3063 sec/batch\n",
      "Epoch: 3/20...  Training Step: 6100...  Training loss: 1.3739...  0.3069 sec/batch\n",
      "Epoch: 3/20...  Training Step: 6200...  Training loss: 1.3192...  0.3066 sec/batch\n",
      "Epoch: 3/20...  Training Step: 6300...  Training loss: 1.2996...  0.3063 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6400...  Training loss: 1.3144...  0.3061 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6500...  Training loss: 1.3405...  0.3062 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6600...  Training loss: 1.3046...  0.3065 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6700...  Training loss: 1.3062...  0.3061 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6800...  Training loss: 1.2808...  0.3050 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6900...  Training loss: 1.3108...  0.3058 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7000...  Training loss: 1.3476...  0.3065 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7100...  Training loss: 1.3092...  0.3059 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7200...  Training loss: 1.2832...  0.3061 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7300...  Training loss: 1.2885...  0.3064 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7400...  Training loss: 1.2906...  0.3063 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7500...  Training loss: 1.2893...  0.3057 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7600...  Training loss: 1.2963...  0.3056 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7700...  Training loss: 1.2994...  0.3060 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7800...  Training loss: 1.2943...  0.3065 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7900...  Training loss: 1.2954...  0.3064 sec/batch\n",
      "Epoch: 4/20...  Training Step: 8000...  Training loss: 1.3172...  0.3061 sec/batch\n",
      "Epoch: 4/20...  Training Step: 8100...  Training loss: 1.2756...  0.3059 sec/batch\n",
      "Epoch: 4/20...  Training Step: 8200...  Training loss: 1.2805...  0.3062 sec/batch\n",
      "Epoch: 4/20...  Training Step: 8300...  Training loss: 1.2809...  0.3068 sec/batch\n",
      "Epoch: 4/20...  Training Step: 8400...  Training loss: 1.3026...  0.3052 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8500...  Training loss: 1.2706...  0.3056 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8600...  Training loss: 1.2910...  0.3061 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8700...  Training loss: 1.3234...  0.3056 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8800...  Training loss: 1.3074...  0.3064 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8900...  Training loss: 1.2887...  0.3061 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9000...  Training loss: 1.2966...  0.3058 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9100...  Training loss: 1.2955...  0.3062 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9200...  Training loss: 1.2574...  0.3061 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9300...  Training loss: 1.2679...  0.3064 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9400...  Training loss: 1.2477...  0.3058 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9500...  Training loss: 1.2972...  0.3059 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9600...  Training loss: 1.2788...  0.3054 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9700...  Training loss: 1.2808...  0.3068 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9800...  Training loss: 1.2485...  0.3075 sec/batch\n",
      "Epoch: 5/20...  Training Step: 9900...  Training loss: 1.2563...  0.3060 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 10000...  Training loss: 1.2531...  0.3061 sec/batch\n",
      "Epoch: 5/20...  Training Step: 10100...  Training loss: 1.2980...  0.3059 sec/batch\n",
      "Epoch: 5/20...  Training Step: 10200...  Training loss: 1.3279...  0.3060 sec/batch\n",
      "Epoch: 5/20...  Training Step: 10300...  Training loss: 1.2704...  0.3066 sec/batch\n",
      "Epoch: 5/20...  Training Step: 10400...  Training loss: 1.2448...  0.3065 sec/batch\n",
      "Epoch: 5/20...  Training Step: 10500...  Training loss: 1.2523...  0.3054 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10600...  Training loss: 1.3056...  0.3061 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10700...  Training loss: 1.2804...  0.3054 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10800...  Training loss: 1.2678...  0.3072 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10900...  Training loss: 1.2402...  0.3066 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11000...  Training loss: 1.2749...  0.3063 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11100...  Training loss: 1.2682...  0.3056 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11200...  Training loss: 1.2748...  0.3073 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11300...  Training loss: 1.2708...  0.3064 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11400...  Training loss: 1.2751...  0.3059 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11500...  Training loss: 1.2633...  0.3054 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11600...  Training loss: 1.2719...  0.3061 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11700...  Training loss: 1.2485...  0.3070 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11800...  Training loss: 1.2231...  0.3071 sec/batch\n",
      "Epoch: 6/20...  Training Step: 11900...  Training loss: 1.2454...  0.3057 sec/batch\n",
      "Epoch: 6/20...  Training Step: 12000...  Training loss: 1.2379...  0.3057 sec/batch\n",
      "Epoch: 6/20...  Training Step: 12100...  Training loss: 1.2241...  0.3071 sec/batch\n",
      "Epoch: 6/20...  Training Step: 12200...  Training loss: 1.2545...  0.3065 sec/batch\n",
      "Epoch: 6/20...  Training Step: 12300...  Training loss: 1.2842...  0.3056 sec/batch\n",
      "Epoch: 6/20...  Training Step: 12400...  Training loss: 1.2474...  0.3062 sec/batch\n",
      "Epoch: 6/20...  Training Step: 12500...  Training loss: 1.2260...  0.3062 sec/batch\n",
      "Epoch: 6/20...  Training Step: 12600...  Training loss: 1.2773...  0.3054 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12700...  Training loss: 1.2427...  0.3064 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12800...  Training loss: 1.2519...  0.3052 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12900...  Training loss: 1.2300...  0.3066 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13000...  Training loss: 1.2503...  0.3065 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13100...  Training loss: 1.2345...  0.3055 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13200...  Training loss: 1.2723...  0.3071 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13300...  Training loss: 1.2694...  0.3064 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13400...  Training loss: 1.1913...  0.3064 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13500...  Training loss: 1.2494...  0.3074 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13600...  Training loss: 1.2538...  0.3069 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13700...  Training loss: 1.2440...  0.3058 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13800...  Training loss: 1.2618...  0.3066 sec/batch\n",
      "Epoch: 7/20...  Training Step: 13900...  Training loss: 1.2570...  0.3072 sec/batch\n",
      "Epoch: 7/20...  Training Step: 14000...  Training loss: 1.2606...  0.3066 sec/batch\n",
      "Epoch: 7/20...  Training Step: 14100...  Training loss: 1.2071...  0.3059 sec/batch\n",
      "Epoch: 7/20...  Training Step: 14200...  Training loss: 1.2448...  0.3066 sec/batch\n",
      "Epoch: 7/20...  Training Step: 14300...  Training loss: 1.2328...  0.3070 sec/batch\n",
      "Epoch: 7/20...  Training Step: 14400...  Training loss: 1.2653...  0.3060 sec/batch\n",
      "Epoch: 7/20...  Training Step: 14500...  Training loss: 1.2608...  0.3055 sec/batch\n",
      "Epoch: 7/20...  Training Step: 14600...  Training loss: 1.2330...  0.3058 sec/batch\n",
      "Epoch: 7/20...  Training Step: 14700...  Training loss: 1.2366...  0.3063 sec/batch\n",
      "Epoch: 8/20...  Training Step: 14800...  Training loss: 1.2176...  0.3050 sec/batch\n",
      "Epoch: 8/20...  Training Step: 14900...  Training loss: 1.2975...  0.3063 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15000...  Training loss: 1.1891...  0.3053 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15100...  Training loss: 1.2116...  0.3062 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15200...  Training loss: 1.2324...  0.3068 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15300...  Training loss: 1.2523...  0.3064 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15400...  Training loss: 1.2503...  0.3058 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15500...  Training loss: 1.2318...  0.3063 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15600...  Training loss: 1.2341...  0.3073 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15700...  Training loss: 1.2480...  0.3063 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15800...  Training loss: 1.2603...  0.3064 sec/batch\n",
      "Epoch: 8/20...  Training Step: 15900...  Training loss: 1.2215...  0.3055 sec/batch\n",
      "Epoch: 8/20...  Training Step: 16000...  Training loss: 1.2200...  0.3063 sec/batch\n",
      "Epoch: 8/20...  Training Step: 16100...  Training loss: 1.2403...  0.3064 sec/batch\n",
      "Epoch: 8/20...  Training Step: 16200...  Training loss: 1.1995...  0.3063 sec/batch\n",
      "Epoch: 8/20...  Training Step: 16300...  Training loss: 1.2496...  0.3057 sec/batch\n",
      "Epoch: 8/20...  Training Step: 16400...  Training loss: 1.2041...  0.3063 sec/batch\n",
      "Epoch: 8/20...  Training Step: 16500...  Training loss: 1.2996...  0.3060 sec/batch\n",
      "Epoch: 8/20...  Training Step: 16600...  Training loss: 1.2661...  0.3060 sec/batch\n",
      "Epoch: 8/20...  Training Step: 16700...  Training loss: 1.2666...  0.3065 sec/batch\n",
      "Epoch: 8/20...  Training Step: 16800...  Training loss: 1.2385...  0.3065 sec/batch\n",
      "Epoch: 9/20...  Training Step: 16900...  Training loss: 1.2551...  0.3061 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17000...  Training loss: 1.2206...  0.3063 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17100...  Training loss: 1.2462...  0.3058 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17200...  Training loss: 1.2235...  0.3059 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17300...  Training loss: 1.2223...  0.3056 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17400...  Training loss: 1.2669...  0.3063 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17500...  Training loss: 1.2115...  0.3059 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17600...  Training loss: 1.2229...  0.3049 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17700...  Training loss: 1.2680...  0.3056 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17800...  Training loss: 1.2731...  0.3050 sec/batch\n",
      "Epoch: 9/20...  Training Step: 17900...  Training loss: 1.2376...  0.3061 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18000...  Training loss: 1.2169...  0.3056 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18100...  Training loss: 1.2163...  0.3064 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18200...  Training loss: 1.2316...  0.3060 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18300...  Training loss: 1.2140...  0.3067 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18400...  Training loss: 1.2155...  0.3062 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18500...  Training loss: 1.2396...  0.3067 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18600...  Training loss: 1.2532...  0.3060 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18700...  Training loss: 1.2419...  0.3065 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18800...  Training loss: 1.2372...  0.3064 sec/batch\n",
      "Epoch: 9/20...  Training Step: 18900...  Training loss: 1.2372...  0.3057 sec/batch\n",
      "Epoch: 10/20...  Training Step: 19000...  Training loss: 1.1947...  0.3076 sec/batch\n",
      "Epoch: 10/20...  Training Step: 19100...  Training loss: 1.2179...  0.3059 sec/batch\n",
      "Epoch: 10/20...  Training Step: 19200...  Training loss: 1.2223...  0.3057 sec/batch\n",
      "Epoch: 10/20...  Training Step: 19300...  Training loss: 1.2640...  0.3059 sec/batch\n",
      "Epoch: 10/20...  Training Step: 19400...  Training loss: 1.2637...  0.3053 sec/batch\n",
      "Epoch: 10/20...  Training Step: 19500...  Training loss: 1.2431...  0.3059 sec/batch\n",
      "Epoch: 10/20...  Training Step: 19600...  Training loss: 1.2773...  0.3063 sec/batch\n",
      "Epoch: 10/20...  Training Step: 19700...  Training loss: 1.2428...  0.3063 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 19800...  Training loss: 1.2352...  0.3072 sec/batch\n",
      "Epoch: 10/20...  Training Step: 19900...  Training loss: 1.2300...  0.3058 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20000...  Training loss: 1.2204...  0.3069 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20100...  Training loss: 1.2323...  0.3063 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20200...  Training loss: 1.2184...  0.3056 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20300...  Training loss: 1.1687...  0.3055 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20400...  Training loss: 1.2148...  0.3063 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20500...  Training loss: 1.2280...  0.3065 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20600...  Training loss: 1.2194...  0.3061 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20700...  Training loss: 1.2448...  0.3063 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20800...  Training loss: 1.2560...  0.3057 sec/batch\n",
      "Epoch: 10/20...  Training Step: 20900...  Training loss: 1.2237...  0.3055 sec/batch\n",
      "Epoch: 10/20...  Training Step: 21000...  Training loss: 1.2469...  0.3058 sec/batch\n",
      "Epoch: 10/20...  Training Step: 21100...  Training loss: 1.3433...  0.3069 sec/batch\n",
      "Epoch: 11/20...  Training Step: 21200...  Training loss: 1.2431...  0.3063 sec/batch\n",
      "Epoch: 11/20...  Training Step: 21300...  Training loss: 1.2279...  0.3056 sec/batch\n",
      "Epoch: 11/20...  Training Step: 21400...  Training loss: 1.2634...  0.3060 sec/batch\n",
      "Epoch: 11/20...  Training Step: 21500...  Training loss: 1.2049...  0.3063 sec/batch\n",
      "Epoch: 11/20...  Training Step: 21600...  Training loss: 1.2101...  0.3061 sec/batch\n",
      "Epoch: 11/20...  Training Step: 21700...  Training loss: 1.2326...  0.3063 sec/batch\n",
      "Epoch: 11/20...  Training Step: 21800...  Training loss: 1.2099...  0.3060 sec/batch\n",
      "Epoch: 11/20...  Training Step: 21900...  Training loss: 1.1974...  0.3057 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22000...  Training loss: 1.1775...  0.3061 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22100...  Training loss: 1.2099...  0.3064 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22200...  Training loss: 1.1981...  0.3062 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22300...  Training loss: 1.2254...  0.3066 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22400...  Training loss: 1.2136...  0.3057 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22500...  Training loss: 1.2242...  0.3059 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22600...  Training loss: 1.2237...  0.3065 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22700...  Training loss: 1.2107...  0.3060 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22800...  Training loss: 1.2843...  0.3056 sec/batch\n",
      "Epoch: 11/20...  Training Step: 22900...  Training loss: 1.2821...  0.3068 sec/batch\n",
      "Epoch: 11/20...  Training Step: 23000...  Training loss: 1.2118...  0.3063 sec/batch\n",
      "Epoch: 11/20...  Training Step: 23100...  Training loss: 1.2266...  0.3059 sec/batch\n",
      "Epoch: 11/20...  Training Step: 23200...  Training loss: 1.2538...  0.3064 sec/batch\n",
      "Epoch: 12/20...  Training Step: 23300...  Training loss: 1.2147...  0.3067 sec/batch\n",
      "Epoch: 12/20...  Training Step: 23400...  Training loss: 1.2236...  0.3055 sec/batch\n",
      "Epoch: 12/20...  Training Step: 23500...  Training loss: 1.2344...  0.3065 sec/batch\n",
      "Epoch: 12/20...  Training Step: 23600...  Training loss: 1.2020...  0.3071 sec/batch\n",
      "Epoch: 12/20...  Training Step: 23700...  Training loss: 1.2175...  0.3064 sec/batch\n",
      "Epoch: 12/20...  Training Step: 23800...  Training loss: 1.2260...  0.3061 sec/batch\n",
      "Epoch: 12/20...  Training Step: 23900...  Training loss: 1.2487...  0.3065 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24000...  Training loss: 1.2345...  0.3065 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24100...  Training loss: 1.2182...  0.3064 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24200...  Training loss: 1.2388...  0.3065 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24300...  Training loss: 1.2340...  0.3067 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24400...  Training loss: 1.2052...  0.3060 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24500...  Training loss: 1.2165...  0.3057 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24600...  Training loss: 1.2225...  0.3077 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24700...  Training loss: 1.1931...  0.3060 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24800...  Training loss: 1.2339...  0.3063 sec/batch\n",
      "Epoch: 12/20...  Training Step: 24900...  Training loss: 1.2115...  0.3063 sec/batch\n",
      "Epoch: 12/20...  Training Step: 25000...  Training loss: 1.2536...  0.3063 sec/batch\n",
      "Epoch: 12/20...  Training Step: 25100...  Training loss: 1.2355...  0.3059 sec/batch\n",
      "Epoch: 12/20...  Training Step: 25200...  Training loss: 1.1962...  0.3058 sec/batch\n",
      "Epoch: 12/20...  Training Step: 25300...  Training loss: 1.1722...  0.3063 sec/batch\n",
      "Epoch: 13/20...  Training Step: 25400...  Training loss: 1.2039...  0.3070 sec/batch\n",
      "Epoch: 13/20...  Training Step: 25500...  Training loss: 1.2113...  0.3053 sec/batch\n",
      "Epoch: 13/20...  Training Step: 25600...  Training loss: 1.2002...  0.3061 sec/batch\n",
      "Epoch: 13/20...  Training Step: 25700...  Training loss: 1.2238...  0.3052 sec/batch\n",
      "Epoch: 13/20...  Training Step: 25800...  Training loss: 1.2065...  0.3060 sec/batch\n",
      "Epoch: 13/20...  Training Step: 25900...  Training loss: 1.2113...  0.3067 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26000...  Training loss: 1.2148...  0.3058 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26100...  Training loss: 1.1926...  0.3060 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26200...  Training loss: 1.2178...  0.3056 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26300...  Training loss: 1.2156...  0.3062 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26400...  Training loss: 1.2292...  0.3063 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26500...  Training loss: 1.1828...  0.3063 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26600...  Training loss: 1.2105...  0.3056 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26700...  Training loss: 1.2339...  0.3052 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26800...  Training loss: 1.2321...  0.3063 sec/batch\n",
      "Epoch: 13/20...  Training Step: 26900...  Training loss: 1.2231...  0.3070 sec/batch\n",
      "Epoch: 13/20...  Training Step: 27000...  Training loss: 1.2497...  0.3065 sec/batch\n",
      "Epoch: 13/20...  Training Step: 27100...  Training loss: 1.2155...  0.3059 sec/batch\n",
      "Epoch: 13/20...  Training Step: 27200...  Training loss: 1.2525...  0.3058 sec/batch\n",
      "Epoch: 13/20...  Training Step: 27300...  Training loss: 1.2128...  0.3064 sec/batch\n",
      "Epoch: 13/20...  Training Step: 27400...  Training loss: 1.2059...  0.3061 sec/batch\n",
      "Epoch: 14/20...  Training Step: 27500...  Training loss: 1.2003...  0.3060 sec/batch\n",
      "Epoch: 14/20...  Training Step: 27600...  Training loss: 1.2443...  0.3070 sec/batch\n",
      "Epoch: 14/20...  Training Step: 27700...  Training loss: 1.2028...  0.3064 sec/batch\n",
      "Epoch: 14/20...  Training Step: 27800...  Training loss: 1.1928...  0.3069 sec/batch\n",
      "Epoch: 14/20...  Training Step: 27900...  Training loss: 1.1955...  0.3058 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28000...  Training loss: 1.2122...  0.3066 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28100...  Training loss: 1.2388...  0.3062 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28200...  Training loss: 1.2100...  0.3064 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28300...  Training loss: 1.2003...  0.3063 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28400...  Training loss: 1.2036...  0.3066 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28500...  Training loss: 1.2084...  0.3069 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28600...  Training loss: 1.2058...  0.3057 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28700...  Training loss: 1.2055...  0.3065 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28800...  Training loss: 1.2148...  0.3069 sec/batch\n",
      "Epoch: 14/20...  Training Step: 28900...  Training loss: 1.2054...  0.3060 sec/batch\n",
      "Epoch: 14/20...  Training Step: 29000...  Training loss: 1.2141...  0.3064 sec/batch\n",
      "Epoch: 14/20...  Training Step: 29100...  Training loss: 1.2211...  0.3066 sec/batch\n",
      "Epoch: 14/20...  Training Step: 29200...  Training loss: 1.1893...  0.3065 sec/batch\n",
      "Epoch: 14/20...  Training Step: 29300...  Training loss: 1.1987...  0.3059 sec/batch\n",
      "Epoch: 14/20...  Training Step: 29400...  Training loss: 1.1967...  0.3061 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 29500...  Training loss: 1.2105...  0.3057 sec/batch\n",
      "Epoch: 15/20...  Training Step: 29600...  Training loss: 1.1834...  0.3056 sec/batch\n",
      "Epoch: 15/20...  Training Step: 29700...  Training loss: 1.2034...  0.3067 sec/batch\n",
      "Epoch: 15/20...  Training Step: 29800...  Training loss: 1.2421...  0.3064 sec/batch\n",
      "Epoch: 15/20...  Training Step: 29900...  Training loss: 1.2304...  0.3064 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30000...  Training loss: 1.2019...  0.3058 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30100...  Training loss: 1.2211...  0.3055 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30200...  Training loss: 1.2211...  0.3056 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30300...  Training loss: 1.1866...  0.3065 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30400...  Training loss: 1.1887...  0.3060 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30500...  Training loss: 1.1720...  0.3068 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30600...  Training loss: 1.2281...  0.3066 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30700...  Training loss: 1.2022...  0.3067 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30800...  Training loss: 1.2058...  0.3061 sec/batch\n",
      "Epoch: 15/20...  Training Step: 30900...  Training loss: 1.1837...  0.3064 sec/batch\n",
      "Epoch: 15/20...  Training Step: 31000...  Training loss: 1.1903...  0.3065 sec/batch\n",
      "Epoch: 15/20...  Training Step: 31100...  Training loss: 1.1870...  0.3058 sec/batch\n",
      "Epoch: 15/20...  Training Step: 31200...  Training loss: 1.2348...  0.3058 sec/batch\n",
      "Epoch: 15/20...  Training Step: 31300...  Training loss: 1.2496...  0.3059 sec/batch\n",
      "Epoch: 15/20...  Training Step: 31400...  Training loss: 1.1985...  0.3065 sec/batch\n",
      "Epoch: 15/20...  Training Step: 31500...  Training loss: 1.1742...  0.3061 sec/batch\n",
      "Epoch: 15/20...  Training Step: 31600...  Training loss: 1.1818...  0.3059 sec/batch\n",
      "Epoch: 16/20...  Training Step: 31700...  Training loss: 1.2390...  0.3069 sec/batch\n",
      "Epoch: 16/20...  Training Step: 31800...  Training loss: 1.2252...  0.3077 sec/batch\n",
      "Epoch: 16/20...  Training Step: 31900...  Training loss: 1.2003...  0.3067 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32000...  Training loss: 1.1733...  0.3069 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32100...  Training loss: 1.2121...  0.3064 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32200...  Training loss: 1.2004...  0.3062 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32300...  Training loss: 1.2028...  0.3064 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32400...  Training loss: 1.2024...  0.3064 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32500...  Training loss: 1.2119...  0.3071 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32600...  Training loss: 1.2099...  0.3062 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32700...  Training loss: 1.2153...  0.3068 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32800...  Training loss: 1.2035...  0.3056 sec/batch\n",
      "Epoch: 16/20...  Training Step: 32900...  Training loss: 1.1751...  0.3062 sec/batch\n",
      "Epoch: 16/20...  Training Step: 33000...  Training loss: 1.1863...  0.3063 sec/batch\n",
      "Epoch: 16/20...  Training Step: 33100...  Training loss: 1.1799...  0.3065 sec/batch\n",
      "Epoch: 16/20...  Training Step: 33200...  Training loss: 1.1799...  0.3063 sec/batch\n",
      "Epoch: 16/20...  Training Step: 33300...  Training loss: 1.1930...  0.3067 sec/batch\n",
      "Epoch: 16/20...  Training Step: 33400...  Training loss: 1.2299...  0.3054 sec/batch\n",
      "Epoch: 16/20...  Training Step: 33500...  Training loss: 1.1930...  0.3067 sec/batch\n",
      "Epoch: 16/20...  Training Step: 33600...  Training loss: 1.1752...  0.3061 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b9e6bb026331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                   model.optimizer],\n\u001b[0;32m---> 26\u001b[0;31m                                                  feed_dict=feed)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every_n\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers,\n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state,  _ = sess.run([model.loss,\n",
    "                                                  model.final_state,\n",
    "                                                  model.optimizer],\n",
    "                                                 feed_dict=feed)\n",
    "\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e + 1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end - start)))\n",
    "\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i33600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i13800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i14000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i14200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i14400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i14600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i14800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i15000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i15200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i15400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i15600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i15800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33600_l512.ckpt\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i33600_l512.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i33600_l512.ckpt\n",
      "poop are the only reason i want to store\n",
      "hep: loool\n",
      "ryan: i wonder what his children is back\n",
      "rene: lol\n",
      "david: i have no idea what to think is i wont be free in a long time\n",
      "hep: lol\n",
      "hep: i have to go away\n",
      "david: then we are already sours but not in an explusion in the face\n",
      "ryan: the campun all sense is about sami well i dont want to be a last work\n",
      "samantha: it's a little bad at this thing\n",
      "ryan: they did today they dont have a couple days out i did\n",
      "samantha: i'd have a little stuff\n",
      "samantha: sorry if i was the born we will hung out i can't even die in my building i think\n",
      "ryan: i want to see those\n",
      "ryan: thong is still the one that helps\n",
      "ryan: she didnt, i was like \"yeah i'm going to go at this bed this minute, and his brother's answer then they didn't like them\"\n",
      "samantha: i thought hidestays are sure tho\n",
      "david: lol\n",
      "ryan: i didnt\n",
      "ryan: thats not supposed to be this time\n",
      "david: it should just be famous\n",
      "ryan: also i think it didnt get to my faverint order\n",
      "samantha: how much are you growing\n",
      "ryan: i thought about how the stuff is still the only word\n",
      "ryan: i was laughing the christmas color\n",
      "ryan: thats the problem\n",
      "david: i don't like the street\n",
      "ryan: that's a good person\n",
      "ryan: and i dont wanna stay outside that shit too, too bad\n",
      "ryan: its not gonna be an image\n",
      "ryan: and this is where they are\n",
      "ryan: it should be french thangage\n",
      "david: and hid their convinces with the terrible person to draw a seat into a police\n",
      "samantha: then i clicked this one\n",
      "samantha: i think these cat is too much for me\n",
      "ryan: i wonder why i was like \"why did he say a stuff around him and it was too late\"\n",
      "ryan: i drink it and i cant take the club or something but i didnt whine about the top\n",
      "ryan: it was true\n",
      "ryan: and it was a good idea\n",
      "ryan: sami is going to take a speaker\n",
      "ryan: im glad he has a problem\n",
      "ryan: she's the bottle of the stop and i could treat that\n",
      "david: that's what i said\n",
      "ryan: it was too bad but i want a potato then\n",
      "ryan: and i dont want to get to the teenager\n",
      "leila: she said \"here isn't this\"\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"poop\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
